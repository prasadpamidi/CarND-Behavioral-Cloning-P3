{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os.path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda, Dropout\n",
    "from keras.layers import Cropping2D, Convolution2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load track data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(directory, correction_factor):\n",
    "    \"\"\"\n",
    "    Read the data csv file, generates image paths and measurements.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    csv_file_path = directory + '/driving_log.csv'\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        return []\n",
    "\n",
    "    with open(csv_file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        for line in reader:\n",
    "            lines.append(line)\n",
    "    \n",
    "    processed_results = []\n",
    "\n",
    "    images_dir = directory + '/IMG/'\n",
    "    if not os.path.exists(images_dir):\n",
    "        return processed_results\n",
    "\n",
    "    for line in lines:\n",
    "        steering_center = float(line[3])\n",
    "        steering_left = steering_center + correction_factor\n",
    "        steering_right = steering_center - correction_factor\n",
    "\n",
    "        img_center_path = images_dir + line[0].split('/')[-1]\n",
    "        img_left_path = images_dir + line[1].split('/')[-1]\n",
    "        img_right_path = images_dir + line[2].split('/')[-1]\n",
    "\n",
    "        processed_results.append((img_center_path, steering_center))\n",
    "        processed_results.append((img_left_path, steering_left))\n",
    "        processed_results.append((img_right_path, steering_right))\n",
    "\n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution with 2 straight laps before augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Figure' object has no attribute 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-038f40c354d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# plt.bar(y_pos, unique_rotation_angle_counts , align='center', width=0.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# plt.xticks(y_pos, unique_rotation_angles, rotation='vertical')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Steering rotation angles data distribution'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Figure' object has no attribute 'title'"
     ]
    }
   ],
   "source": [
    "plt.rcdefaults()\n",
    "%matplotlib inline\n",
    "\n",
    "samples = process_data('data', 0.25)\n",
    "unique_rotation_angles, unique_rotation_angle_counts = np.unique([x[1] for x in samples], return_counts=True)\n",
    "\n",
    "# y_pos = np.arange(len(unique_rotation_angles))\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([min(unique_rotation_angles), 0.1, len(unique_rotation_angles), max(unique_rotation_angle_counts)])\n",
    "axes.plot(unique_rotation_angles, unique_rotation_angle_counts)\n",
    "\n",
    "# plt.bar(y_pos, unique_rotation_angle_counts , align='center', width=0.5)\n",
    "# plt.xticks(y_pos, unique_rotation_angles, rotation='vertical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator for train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(samples, batch_size=32):\n",
    "    \"\"\"\n",
    "    Returns a generator for the required images and augmented images from the given set of samples.\n",
    "    \"\"\"\n",
    "    num_samples = len(samples)\n",
    "    while 1:\n",
    "        samples = shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            augmented_images, augmented_measurements = [], []\n",
    "\n",
    "            for image_path, measurement in batch_samples:\n",
    "                image = cv2.imread(image_path)\n",
    "                augmented_images.append(image)\n",
    "                augmented_measurements.append(measurement)\n",
    "                augmented_images.append(cv2.flip(image, 1))\n",
    "                augmented_measurements.append(measurement * -1.0)\n",
    "\n",
    "            x_train = np.array(augmented_images)\n",
    "            y_train = np.array(augmented_measurements)\n",
    "\n",
    "            yield shuffle(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing keras layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_preprocessing_layers(model):\n",
    "    \"\"\"\n",
    "    Apply some common data preprocessing layers to the keras model.\n",
    "    \"\"\"\n",
    "    model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(160, 320, 3)))\n",
    "    model.add(Cropping2D(cropping=((70, 25), (0, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with LeNET Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lenet_arch_model():\n",
    "    \"\"\"\n",
    "    Returns the keras model for the LENET architecture.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    add_preprocessing_layers(model)\n",
    "\n",
    "    model.add(Convolution2D(6, kernel_size=(5, 5), padding=\"valid\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Convolution2D(16, kernel_size=(5, 5), padding=\"valid\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(120, activation='relu'))\n",
    "    model.add(Dropout(0.65))\n",
    "    model.add(Dense(84, activation='relu'))\n",
    "    model.add(Dropout(0.65))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with NVIDIA Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nvidia_arch_model():\n",
    "    \"\"\"\n",
    "    Returns the keras model for the popular NVIDIA architecture.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    add_preprocessing_layers(model)\n",
    "\n",
    "    model.add(Convolution2D(24, 5, strides=(2, 2), activation='relu'))\n",
    "    model.add(Convolution2D(36, 5, strides=(2, 2), activation='relu'))\n",
    "    model.add(Convolution2D(48, 5, strides=(2, 2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100))\n",
    "    model.add(Dense(50))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a Keras Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_model_loss(hist_object):\n",
    "    \"\"\"\n",
    "    Visualize the loss metrics for the keras model .\n",
    "    \"\"\"\n",
    "    plt.plot(hist_object.history['loss'])\n",
    "    plt.plot(hist_object.history['val_loss'])\n",
    "    plt.title('Mean Squared Error Loss')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training set', 'Validation set'], loc='upper right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When using a generator for validation data, you must specify a value for `validation_steps`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ba4eb5b2f291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                                            \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                                            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                                            callbacks=keras_model_callbacks())\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tittu/anaconda/envs/py352/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tittu/anaconda/envs/py352/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1115\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tittu/anaconda/envs/py352/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tittu/anaconda/envs/py352/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                    isinstance(validation_data, Sequence))\n\u001b[1;32m   1734\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval_gen\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             raise ValueError('When using a generator for validation data, '\n\u001b[0m\u001b[1;32m   1736\u001b[0m                              \u001b[0;34m'you must specify a value for '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                              '`validation_steps`.')\n",
      "\u001b[0;31mValueError\u001b[0m: When using a generator for validation data, you must specify a value for `validation_steps`."
     ]
    }
   ],
   "source": [
    "KERAS_CHECKPOINT_FILE_PATH = 'keras.weights.best.hdf5'\n",
    "\n",
    "def keras_model_callbacks():\n",
    "    \"\"\"\n",
    "    Returns an array of keras checkpoint callback.\n",
    "    \"\"\"\n",
    "    return [ModelCheckpoint(KERAS_CHECKPOINT_FILE_PATH,\n",
    "                            monitor='val_acc',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='max')]\n",
    "\n",
    "### Splitting samples and creating generators.\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_samples = process_data('data', 0.2)\n",
    "train_samples, validation_samples = train_test_split(data_samples, test_size=0.2)\n",
    "\n",
    "### Create seperate generators for training and validation data\n",
    "train_generator = generator(train_samples, batch_size=BATCH_SIZE)\n",
    "validation_generator = generator(validation_samples, batch_size=BATCH_SIZE)\n",
    "\n",
    "### Create a keras model\n",
    "keras_model = nvidia_arch_model()\n",
    "\n",
    "### Load any previous saved checkpoint weights, if exists\n",
    "if os.path.exists(KERAS_CHECKPOINT_FILE_PATH):\n",
    "    keras_model.load_weights(KERAS_CHECKPOINT_FILE_PATH)\n",
    "else:\n",
    "    print(\"No prior model checkpoints exist\")\n",
    "\n",
    "### Compile and train the model using the generator function\n",
    "keras_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history_object = keras_model.fit_generator(train_generator,\n",
    "                                           steps_per_epoch=int(np.floor((len(train_samples))/BATCH_SIZE)*BATCH_SIZE),\n",
    "                                           validation_data=validation_generator,\n",
    "                                           validation_steps=int(np.floor((len(validation_samples))/BATCH_SIZE)*BATCH_SIZE),\n",
    "                                           epochs=5,\n",
    "                                           verbose=1,\n",
    "                                           callbacks=keras_model_callbacks())\n",
    "keras_model.summary()\n",
    "\n",
    "### Generate visualiaztion of the entire model\n",
    "plot_model(keras_model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "### Save the trained model\n",
    "keras_model.save('model.h5')\n",
    "\n",
    "### Visualize the model loss over training and validation data - NOT NEEDED FOR EC2\n",
    "#visualize_model_loss(history_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
